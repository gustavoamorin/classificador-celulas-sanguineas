{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 29380,
          "sourceType": "datasetVersion",
          "datasetId": 9232
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v7-dAZPa-x-g",
        "dxFsIcOz-pcE",
        "VvU3ZkLx-pcF"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **data_set do kaggle**"
      ],
      "metadata": {
        "id": "v7-dAZPa-x-g"
      }
    },
    {
      "source": [
        "# IMPORTANTE: RODE ESTA CÉLULA PARA IMPORTAR SUAS FONTES DE DADOS DO KAGGLE,\n",
        "# DEPOIS, SINTA-SE À VONTADE PARA DELETAR ESTA CÉLULA.\n",
        "# NOTA: O AMBIENTE DESTE NOTEBOOK É DIFERENTE DO AMBIENTE PYTHON DO KAGGLE,\n",
        "# PORTANTO, PODEM FALTAR BIBLIOTECAS USADAS PELO SEU NOTEBOOK.\n",
        "import kagglehub\n",
        "paultimothymooney_blood_cells_path = kagglehub.dataset_download('paultimothymooney/blood-cells')\n",
        "\n",
        "print('Importação da fonte de dados concluída.')"
      ],
      "metadata": {
        "id": "r1QHGKnz-pcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45e34ba-64cc-429c-9257-28cd22ef6614"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalação de Dependências**"
      ],
      "metadata": {
        "id": "qMgqux4K-svx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-cv"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:07:58.678997Z",
          "iopub.execute_input": "2025-07-15T17:07:58.679378Z",
          "iopub.status.idle": "2025-07-15T17:08:02.492993Z",
          "shell.execute_reply.started": "2025-07-15T17:07:58.679351Z",
          "shell.execute_reply": "2025-07-15T17:08:02.492007Z"
        },
        "id": "NMsYVei1-pcF",
        "outputId": "c3cff078-989b-42e0-f192-4f1ab06aa395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-cv) (25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras-cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras-cv) (4.9.9)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras-cv) (0.3.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (3.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.7.2)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (3.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (4.14.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2025.7.14)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras-cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (2.19.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->keras-cv) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.70.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.9.0\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importação das Bibliotecas**"
      ],
      "metadata": {
        "id": "dhOGzici-pcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:08:05.810185Z",
          "iopub.execute_input": "2025-07-15T17:08:05.810468Z",
          "iopub.status.idle": "2025-07-15T17:08:11.496386Z",
          "shell.execute_reply.started": "2025-07-15T17:08:05.810447Z",
          "shell.execute_reply": "2025-07-15T17:08:11.495488Z"
        },
        "id": "nMyW5umq-pcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03c89d1-3ea0-4687-bbdd-3d413c87f080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "### funcoes"
      ],
      "metadata": {
        "id": "VvU3ZkLx-pcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_imagem(imagem, rotulo):\n",
        "    # tranforma a imagem com o data_augmentation, mas mantem o rotulo dela\n",
        "    imagem = manipulacao_imagem(imagem, training=True)\n",
        "    return imagem, rotulo"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:21:35.712949Z",
          "iopub.execute_input": "2025-07-15T17:21:35.713254Z",
          "iopub.status.idle": "2025-07-15T17:21:35.717872Z",
          "shell.execute_reply.started": "2025-07-15T17:21:35.713237Z",
          "shell.execute_reply": "2025-07-15T17:21:35.717074Z"
        },
        "id": "IBwxYwp5-pcF"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def para_one_hot(imagem, rotulo):\n",
        "    # basicamente ele coloca uma etiqueta na imagem: exemplo: indice = 1 --> [0,1,0,0] para facilitar os calculos depois\n",
        "    rotulo_one_hot = tf.one_hot(tf.cast(rotulo, tf.int32), depth=4)\n",
        "    return imagem, rotulo_one_hot"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:21:38.428363Z",
          "iopub.execute_input": "2025-07-15T17:21:38.428669Z",
          "iopub.status.idle": "2025-07-15T17:21:38.433831Z",
          "shell.execute_reply.started": "2025-07-15T17:21:38.428646Z",
          "shell.execute_reply": "2025-07-15T17:21:38.432643Z"
        },
        "id": "In7Dhyhg-pcG"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparação e Otimização dos Dados**"
      ],
      "metadata": {
        "id": "95gxj3XJ-pcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset"
      ],
      "metadata": {
        "id": "4DrnYGiQ-pcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_de_treino = tf.keras.utils.image_dataset_from_directory(\n",
        "directory = \"/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TRAIN\", #pasta com os arquivos *aqui tem q mudar para o do drive, pq ele n consegue acessar o kaggle desse jeito, pelo menos não q eu saiba.\n",
        "image_size=(128, 128), #redimenciona o tamanho da imagem. no caso, 128x128\n",
        "batch_size=32, #define quantas imagens são carregadas por vez (lote), mais imagems por lotes = mais velocidade, menos imagem por lote = menos velocidade, exemplo batch 8 com 64 imagens = 8 lotes, batch 32 com 64 imagens = 2 lotes.\n",
        "validation_split=0.2, # porcentagem reservada para validação, no caso 20%\n",
        "subset=\"training\", # indica que este conjunto é a parte de treino (e não validação)\n",
        "seed=47, # define uma semente para garantir que a divisão entre treino/validação seja sempre igual\n",
        ")\n",
        "dataset_de_validacao = tf.keras.utils.image_dataset_from_directory(\n",
        "directory = \"/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TRAIN\", #pasta com os arquivos *aqui tem q mudar para o do drive, pq ele n consegue acessar o kaggle desse jeito, pelo menos não q eu saiba.\n",
        "image_size=(128, 128),\n",
        "batch_size=32,\n",
        "validation_split=0.2,\n",
        "subset=\"validation\", # indica que este conjunto é a parte de validação (e não treino)\n",
        "seed=47,)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:21:39.99535Z",
          "iopub.execute_input": "2025-07-15T17:21:39.995663Z",
          "iopub.status.idle": "2025-07-15T17:21:44.689939Z",
          "shell.execute_reply.started": "2025-07-15T17:21:39.99564Z",
          "shell.execute_reply": "2025-07-15T17:21:44.688999Z"
        },
        "id": "172al52Z-pcG",
        "outputId": "c9b925b0-03aa-45f8-e8a5-89c8d39de245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9957 files belonging to 4 classes.\n",
            "Using 7966 files for training.\n",
            "Found 9957 files belonging to 4 classes.\n",
            "Using 1991 files for validation.\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "### manipulação de imagem"
      ],
      "metadata": {
        "id": "OJrRptEw-pcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequência de transformações visuais que serão aplicadas às imagens (geralmente usa data_augmentation)\n",
        "manipulacao_imagem = tf.keras.Sequential([\n",
        "    # Normalização\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "\n",
        "    # Espelhamento aleatório da imagem na horizontal e vertical\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "\n",
        "    # Rotação aleatória\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "\n",
        "    # Zoom aleatório\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "\n",
        "    # Variação aleatória do contraste\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "\n",
        "    # Alteração aleatória do brilho\n",
        "    tf.keras.layers.RandomBrightness(0.2),\n",
        "\n",
        "    # Translação (deslocamento) aleatória horizontal e vertical\n",
        "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "\n",
        "    # Adição de ruído gaussiano com desvio padrão de 0.1\n",
        "    tf.keras.layers.GaussianNoise(0.1),\n",
        "\n",
        "    # Corte aleatório de uma região da imagem (apaga uma pequena parte)\n",
        "    keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2)\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:22:52.606646Z",
          "iopub.status.idle": "2025-07-15T17:22:52.60703Z",
          "shell.execute_reply.started": "2025-07-15T17:22:52.606832Z",
          "shell.execute_reply": "2025-07-15T17:22:52.606848Z"
        },
        "id": "MIMus2Uv-pcH"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "### aumentando o data_set e transformando em one-hot e aplicando otimização com o AUTOTUNE"
      ],
      "metadata": {
        "id": "K1tqfhh-ERk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtra apenas as imagens da classe eosinófilo (rótulo 0)\n",
        "eosinofilo_ds = dataset_de_treino.filter(lambda x, y: tf.reduce_any(tf.equal(y, 0)))\n",
        "# Filtra apenas as imagens da classe monócito (rótulo 2)\n",
        "monocito_ds = dataset_de_treino.filter(lambda x, y: tf.reduce_any(tf.equal(y, 2)))\n",
        "# Filtra apenas as imagens da classe neutrófilo (rótulo 3)\n",
        "neutrofilo_ds = dataset_de_treino.filter(lambda x, y: tf.reduce_any(tf.equal(y, 3)))\n",
        "\n",
        "# Aplica transformações (data augmentation) nas imagens de eosinófilos e neutrófilos, a função esta la emcima\n",
        "eosinofilo_modificado = eosinofilo_ds.map(transformar_imagem)\n",
        "neutrofilo_modificado = neutrofilo_ds.map(transformar_imagem)\n",
        "monocito_modificado = monocito_ds.map(transformar_imagem)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:21:44.691572Z",
          "iopub.execute_input": "2025-07-15T17:21:44.69186Z",
          "iopub.status.idle": "2025-07-15T17:21:47.14664Z",
          "shell.execute_reply.started": "2025-07-15T17:21:44.69184Z",
          "shell.execute_reply": "2025-07-15T17:21:47.146011Z"
        },
        "id": "jXIYzK2v-pcG"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# Habilita otimização automática de performance para carregamento antecipado de dados\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Concatena os dados aumentados ao dataset original de treino\n",
        "dataset_de_treino_aumentado = dataset_de_treino.concatenate(eosinofilo_modificado)\n",
        "dataset_de_treino_aumentado = dataset_de_treino.concatenate(neutrofilo_modificado)\n",
        "dataset_de_treino_aumentado = dataset_de_treino.concatenate(monocito_modificado)\n",
        "\n",
        "# Converte os rótulos inteiros para o formato one-hot\n",
        "dataset_de_treino_final = dataset_de_treino_aumentado.map(para_one_hot)\n",
        "dataset_de_validacao_final = dataset_de_validacao.map(para_one_hot)\n",
        "\n",
        "# Otimizações de performance para o dataset de treino\n",
        "dataset_de_treino_final = dataset_de_treino_final.cache() # armazena os dados na memória (mais rápido)\n",
        "dataset_de_treino_final = dataset_de_treino_final.shuffle(buffer_size=1000, seed=47) # embaralha os dados\n",
        "dataset_de_treino_final = dataset_de_treino_final.prefetch(buffer_size=AUTOTUNE) # pré-carrega lotes de forma paralela\n",
        "\n",
        "# Pré-carrega o dataset de validação também\n",
        "dataset_de_validacao_final = dataset_de_validacao_final.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:23:50.540191Z",
          "iopub.execute_input": "2025-07-15T17:23:50.540918Z",
          "iopub.status.idle": "2025-07-15T17:23:50.579635Z",
          "shell.execute_reply.started": "2025-07-15T17:23:50.540893Z",
          "shell.execute_reply": "2025-07-15T17:23:50.57902Z"
        },
        "id": "s3dI1vrC-pcG"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer machine**"
      ],
      "metadata": {
        "id": "g75dm-0d-pcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importando o MobileNetV2"
      ],
      "metadata": {
        "id": "Te0i2R9m-pcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tamanho_imagem = (128, 128, 3)  # Define o tamanho das imagens: 128x128 pixels e 3 canais de cor (RGB)\n",
        "\n",
        "modelo_base = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=tamanho_imagem,   # Define o formato da entrada para o modelo base\n",
        "    include_top=False,            # Remove a última camada (camada de classificação) do MobileNetV2, pq no nosso caso serao 4 saidas\n",
        "    weights='imagenet'            # Carrega pesos pré-treinados no dataset ImageNet, geralmente os pesos iniciais sao mais genericos e os ultimos mais espeficicos\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:22:52.608286Z",
          "iopub.status.idle": "2025-07-15T17:22:52.608662Z",
          "shell.execute_reply.started": "2025-07-15T17:22:52.608463Z",
          "shell.execute_reply": "2025-07-15T17:22:52.60848Z"
        },
        "id": "tWgvZARZ-pcH"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otimizando o MobileNetV2"
      ],
      "metadata": {
        "id": "OKZ5tFpS-pcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtém a saída do modelo base (MobileNetV2, sem a parte de classificação)\n",
        "saida = modelo_base.output\n",
        "\n",
        "# Aplica um Global Average Pooling para reduzir as dimensões espaciais\n",
        "saida = tf.keras.layers.GlobalAveragePooling2D()(saida)\n",
        "\n",
        "# Aplica Dropout para evitar overfitting(ficar mt com treino e ruim em validacao)\n",
        "saida = tf.keras.layers.Dropout(0.5)(saida)\n",
        "\n",
        "# Camada densa final com 4 neurônios (número de classes) para a predição\n",
        "camada_predicao = tf.keras.layers.Dense(4, kernel_regularizer = tf.keras.regularizers.l2(0.001))(saida)\n",
        "\n",
        "# Cria o modelo final, com entrada do modelo base e saída da camada de predição\n",
        "modelo_final = tf.keras.Model(\n",
        "    inputs=modelo_base.input,\n",
        "    outputs=camada_predicao\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:09:55.66167Z",
          "iopub.execute_input": "2025-07-15T17:09:55.66202Z",
          "iopub.status.idle": "2025-07-15T17:09:55.688878Z",
          "shell.execute_reply.started": "2025-07-15T17:09:55.661992Z",
          "shell.execute_reply": "2025-07-15T17:09:55.688158Z"
        },
        "id": "6pQJktMs-pcH"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "### compilando"
      ],
      "metadata": {
        "id": "NpFP0u0h-pcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_final.compile(\n",
        "    loss=keras_cv.losses.FocalLoss(from_logits=True),         # Define a função de perda como Focal Loss (ajuda em problemas de classes desbalanceadas, no nosso caso ficou desbalanceado por eu ter concatenado la emcima)\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),   # Usa o otimizador Adam com taxa de aprendizado muito baixa\n",
        "    metrics=['accuracy']                                      # Métrica para avaliar o desempenho: acurácia\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:13:54.972004Z",
          "iopub.execute_input": "2025-07-15T17:13:54.972741Z",
          "iopub.status.idle": "2025-07-15T17:13:54.981579Z",
          "shell.execute_reply.started": "2025-07-15T17:13:54.972717Z",
          "shell.execute_reply": "2025-07-15T17:13:54.980952Z"
        },
        "id": "xZU9Cf8W-pcH"
      },
      "outputs": [],
      "execution_count": 54
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuração dos Callbacks**"
      ],
      "metadata": {
        "id": "EzrLW50N-pcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### caminho para salvar o melhor modelo"
      ],
      "metadata": {
        "id": "hdtk0jyg-pcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Novo caminho: cria uma pasta 'meus_modelos' dentro do seu Google Drive\n",
        "caminho_saida = '/content/drive/MyDrive/meus_modelos'\n",
        "\n",
        "# Caminho completo do arquivo a ser salvo\n",
        "caminho_arquivo_modelo = os.path.join(caminho_saida, 'melhor_modelo_transfer_colab.keras')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:19:39.787031Z",
          "iopub.execute_input": "2025-07-15T17:19:39.787739Z",
          "iopub.status.idle": "2025-07-15T17:19:39.791594Z",
          "shell.execute_reply.started": "2025-07-15T17:19:39.787711Z",
          "shell.execute_reply": "2025-07-15T17:19:39.79078Z"
        },
        "id": "D52u07Sh-pcI"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "### funcoes"
      ],
      "metadata": {
        "id": "gd9zEtr7-pcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interrompe o treinamento se a validação parar de melhorar\n",
        "parada = EarlyStopping(\n",
        "    monitor='val_loss',           # Acompanha a perda (loss) no conjunto de validação\n",
        "    patience=4,                   # Espera 4 épocas sem melhoria antes de interromper\n",
        "    restore_best_weights=True     # Restaura os pesos do melhor ponto do treino (não os da última época)\n",
        ")\n",
        "\n",
        "# Reduz a taxa de aprendizado se a validação não melhorar\n",
        "taxa_de_aprendizado = ReduceLROnPlateau(\n",
        "    monitor='val_loss',           # Acompanha a perda de validação\n",
        "    factor=0.5,                   # Reduz a taxa de aprendizado pela metade\n",
        "    patience=2,                   # Espera 2 épocas sem melhoria para agir\n",
        "    verbose=1                     # Mostra mensagem no console ao reduzir\n",
        ")\n",
        "\n",
        "# Salva automaticamente o melhor modelo durante o treinamento\n",
        "salvar_melhor_modelo = ModelCheckpoint(\n",
        "    caminho_arquivo_modelo,       # Caminho onde o modelo será salvo\n",
        "    monitor='val_loss',           # Salva com base na melhor validação\n",
        "    save_best_only=True,          # Salva apenas quando houver melhoria\n",
        "    verbose=1                     # Exibe mensagem ao salvar\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:13:59.184326Z",
          "iopub.execute_input": "2025-07-15T17:13:59.184908Z",
          "iopub.status.idle": "2025-07-15T17:13:59.189355Z",
          "shell.execute_reply.started": "2025-07-15T17:13:59.184882Z",
          "shell.execute_reply": "2025-07-15T17:13:59.1884Z"
        },
        "id": "j88lpYky-pcI"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Treinamento do Modelo**"
      ],
      "metadata": {
        "id": "vBwPjyyv-pcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historico = modelo_final.fit(\n",
        "    dataset_de_treino_final,                                        # Conjunto de dados de treino (com imagens e rótulos one-hot)\n",
        "    validation_data=dataset_de_validacao_final,                     # Conjunto de validação para monitorar o desempenho\n",
        "    epochs=10,                                                      # Número máximo de épocas para treinamento\n",
        "    callbacks=[parada, taxa_de_aprendizado, salvar_melhor_modelo],  # Chamadas automáticas para controle do treino\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:14:31.184352Z",
          "iopub.execute_input": "2025-07-15T17:14:31.184957Z",
          "iopub.status.idle": "2025-07-15T17:14:51.363953Z",
          "shell.execute_reply.started": "2025-07-15T17:14:31.18493Z",
          "shell.execute_reply": "2025-07-15T17:14:51.362747Z"
        },
        "id": "Vukyp1Vg-pcI",
        "outputId": "c9bb4fdd-e1c9-4c8a-fbd6-e0ad16840fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "    496/Unknown \u001b[1m28s\u001b[0m 26ms/step - accuracy: 0.7307 - loss: 0.1096\n",
            "Epoch 1: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 38ms/step - accuracy: 0.7307 - loss: 0.1096 - val_accuracy: 0.8292 - val_loss: 0.1021 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7536 - loss: 0.0991\n",
            "Epoch 2: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 20ms/step - accuracy: 0.7536 - loss: 0.0991 - val_accuracy: 0.8443 - val_loss: 0.0974 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m496/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7609 - loss: 0.0967\n",
            "Epoch 3: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7608 - loss: 0.0968 - val_accuracy: 0.8247 - val_loss: 0.1053 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m495/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7764 - loss: 0.0909\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7763 - loss: 0.0910 - val_accuracy: 0.8403 - val_loss: 0.1028 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m497/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7834 - loss: 0.0907\n",
            "Epoch 5: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.7834 - loss: 0.0907 - val_accuracy: 0.8227 - val_loss: 0.1127 - learning_rate: 5.0000e-06\n",
            "Epoch 6/10\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7847 - loss: 0.0919\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.09276\n",
            "\u001b[1m498/498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.7847 - loss: 0.0919 - val_accuracy: 0.8242 - val_loss: 0.1160 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "execution_count": 55
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ajuste fino"
      ],
      "metadata": {
        "id": "aV4jQxl1-pcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_final.trainable = True      # Permite que todas as camadas do modelo base sejam treináveis, em true a gente pode retreinar o modelo, perdendo os pesos q a gente carregou,para isso nao acontecer a gente congela alguns \"neuronios\"(abaixo)\n",
        "\n",
        "print(\"Número de camadas no modelo base: \", len(modelo_base.layers))\n",
        "\n",
        "ajuste_fino_a_partir = 30  # Congela as camadas até essa posição para ajuste fino\n",
        "\n",
        "# Congela as camadas do modelo base para que não sejam treinadas\n",
        "for layer in modelo_base.layers[:ajuste_fino_a_partir]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T17:08:55.01924Z",
          "iopub.execute_input": "2025-07-15T17:08:55.019531Z",
          "iopub.status.idle": "2025-07-15T17:08:55.02529Z",
          "shell.execute_reply.started": "2025-07-15T17:08:55.019505Z",
          "shell.execute_reply": "2025-07-15T17:08:55.02437Z"
        },
        "id": "T9IG5myq-pcH",
        "outputId": "c89766a3-8807-4b93-c8a8-1ae7156282a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(\"Número de camadas no modelo base: \", len(modelo_base.layers))\\n\\najuste_fino_a_partir = 120  # Congela as camadas até essa posição para ajuste fino\\n\\n# Congela as camadas do modelo base para que não sejam treinadas\\nfor layer in modelo_base.layers[:ajuste_fino_a_partir]:\\n    layer.trainable = False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "execution_count": 53
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualização do Histórico de Treino**"
      ],
      "metadata": {
        "id": "bsWwn6gY-pcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai os resultados do objeto 'historico'\n",
        "acc = historico.history['accuracy']\n",
        "val_acc = historico.history['val_accuracy']\n",
        "loss = historico.history['loss']\n",
        "val_loss = historico.history['val_loss']\n",
        "\n",
        "# Define o alcance das épocas para o eixo X\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "# Cria a figura que conterá nossos dois gráficos\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Gráfico 1: Acurácia de Treinamento vs. Validação\n",
        "plt.subplot(1, 2, 1) # (1 linha, 2 colunas, primeiro gráfico)\n",
        "plt.plot(epochs_range, acc, label='Acurácia de Treino')\n",
        "plt.plot(epochs_range, val_acc, label='Acurácia de Validação')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Acurácia de Treinamento e Validação')\n",
        "\n",
        "# Gráfico 2: Perda de Treinamento vs. Validação\n",
        "plt.subplot(1, 2, 2) # (1 linha, 2 colunas, segundo gráfico)\n",
        "plt.plot(epochs_range, loss, label='Perda de Treino')\n",
        "plt.plot(epochs_range, val_loss, label='Perda de Validação')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Perda de Treinamento e Validação')\n",
        "\n",
        "# Exibe os gráficos\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T16:41:25.941018Z",
          "iopub.execute_input": "2025-07-15T16:41:25.941577Z",
          "iopub.status.idle": "2025-07-15T16:41:26.295498Z",
          "shell.execute_reply.started": "2025-07-15T16:41:25.941551Z",
          "shell.execute_reply": "2025-07-15T16:41:26.294689Z"
        },
        "id": "GKZZGLxT-pcK"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}